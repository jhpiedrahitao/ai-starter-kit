{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.prompts import PromptTemplate, load_prompt\n",
    "from langchain.agents import load_tools ,initialize_agent, AgentExecutor, create_self_ask_with_search_agent\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from utils.sambanova_endpoint import SambaNovaEndpoint, SambaverseEndpoint\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "\n",
    "from langchain.globals import set_verbose\n",
    "\n",
    "#set_verbose(True)\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sambanova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sambeverse llm\n",
    "llm = SambaverseEndpoint(\n",
    "            sambaverse_model_name=\"Mistral/Mistral-7B-Instruct-v0.2\",\n",
    "            model_kwargs={\n",
    "                \"do_sample\": False, \n",
    "                \"max_tokens_to_generate\": 500,\n",
    "                \"temperature\": 0.01,\n",
    "                \"top_p\": 1,\n",
    "                \"process_prompt\": False,\n",
    "                \"select_expert\": \"Mistral-7B-Instruct-v0.2\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "#sambastudio llm\n",
    "#llm = SambaNovaEndpoint(\n",
    "#    model_kwargs={\"do_sample\": False, \"temperature\": 0.0},\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base tools searcha nd calculator\n",
    "tool_names =['llm-math']#,'serpapi',]\n",
    "tools=load_tools(tool_names,llm)\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new tool basic llm\n",
    "class BasicLLMInput(BaseModel):\n",
    "    query: str = Field(description=\"raw user interaction\")\n",
    "\n",
    "\n",
    "def queryLLM(query: str) -> str:\n",
    "    \"\"\"Process a query with an LLM\"\"\"\n",
    "    prompt = load_prompt(os.path.join(kit_dir,\"prompts/llama70b-Q&A.yaml\"))\n",
    "    query = prompt.format(question=query)\n",
    "    return llm.invoke(query)\n",
    "\n",
    "askLLM = StructuredTool.from_function(\n",
    "    func=queryLLM,\n",
    "    name=\"conversational_query\",\n",
    "    description=\"process user input conversation, following conversation without factual checking\",\n",
    "    args_schema=BasicLLMInput,\n",
    "    return_direct=True,\n",
    ")\n",
    "\n",
    "tools.append(askLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define serper tool\n",
    "class SerperInput(BaseModel):\n",
    "    query: str = Field(description=\"google search query\")\n",
    "\n",
    "\n",
    "def querySerper(query: str) -> str:\n",
    "    \"\"\"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\"\"\"\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "    payload = json.dumps({\n",
    "        \"q\": query\n",
    "    })\n",
    "    headers = {\n",
    "        'X-API-KEY': os.environ.get(\"SERPER_API_KEY\"),\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=payload)\n",
    "    prompt = load_prompt(os.path.join(kit_dir, \"prompts/llama70b-SearchAnalysis.yaml\"))\n",
    "    formated_prompt = prompt.format(question=query, context=json.dumps(response.json()))\n",
    "    \n",
    "    return(llm.invoke(formated_prompt))\n",
    "   \n",
    "serper = StructuredTool.from_function(\n",
    "    func=querySerper,\n",
    "    name=\"search_engine\",\n",
    "    description=\"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\",\n",
    "    args_schema=SerperInput,\n",
    "    return_direct=False,\n",
    ") \n",
    "\n",
    "tools.append(serper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenSERP https://github.com/karust/openserp?tab=readme-ov-file\n",
    "#Run docker run -p 127.0.0.1:7000:7000 -it karust/openserp serve -a 0.0.0.0 -p 7000\n",
    "\n",
    "class OpenSerpInput(BaseModel):\n",
    "    query: str = Field(description=\"google search query\")\n",
    "\n",
    "\n",
    "def queryOpenSerp(query: str) -> str:\n",
    "    \"\"\"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\"\"\"\n",
    "    url = \"http://127.0.0.1:7000/google/search\"\n",
    "    params = {\n",
    "        \"lang\": \"EN\",\n",
    "        \"limit\": 10,\n",
    "        \"text\": query\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    " \n",
    "    prompt = load_prompt(os.path.join(kit_dir, \"prompts/llama70b-OpenSearchAnalysis.yaml\"))\n",
    "    formated_prompt = prompt.format(question=query, context=json.dumps(response.json()))\n",
    "    \n",
    "    return(llm.invoke(formated_prompt))\n",
    "   \n",
    "openSerp = StructuredTool.from_function(\n",
    "    func=queryOpenSerp,\n",
    "    name=\"search_engine\",\n",
    "    description=\"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\",\n",
    "    args_schema=OpenSerpInput,\n",
    "    return_direct=False,\n",
    ") \n",
    "\n",
    "tools.append(openSerp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompting experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PREFIX = \"\"\"<s>[INST] Answer the following questions as best you can. You have access to the following tools:\"\"\"\n",
    "\n",
    "FORMAT_INSTRUCTIONS = \"\"\"Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of {tool_names}\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\"\"\"\n",
    "\n",
    "SUFFIX = \"\"\"Begin!\n",
    "\n",
    "Question: {input} \n",
    "Thought:{agent_scratchpad} [/INST]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modified prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PREFIX = \"[INST] <<SYS>> You are smart assistant that selects a function from list of functions based on user questions.\\\n",
    "Run only one Action at a time. \\nAnswer the following questions as best you can. You have access to the following tools:\"\n",
    "\n",
    "FORMAT_INSTRUCTIONS = \"Use the following format:\\n\\n\\\n",
    "Question: the input question you must answer\\n\\\n",
    "Thought: you should always think about what to do\\n\\\n",
    "Action: the action to take, should be one of [Search, Calculator]\\n\\\n",
    "Action Input: the input to the action\\n\\\n",
    "Observation: the result of the action\\n\\\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times, but only one at a time)\\n\\\n",
    "Thought: I now know the final answer\\n\\\n",
    "Final Answer: the final answer to the original input question\"\n",
    "\n",
    "SUFFIX = \"\"\"Stop after each Action call and wait to get the intermediate results, do not make up multiple steps by yourself\n",
    "\n",
    "Begin! <</SYS>>\n",
    "\n",
    "Question: {input} \n",
    "Thought:{agent_scratchpad}[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"\"\"[INST] <<SYS>> You are smart assistant that selects a function from list of functions based on user questions.Run only one Action at a time.\n",
    "You have access to the following tools:\"\"\"\n",
    "\n",
    "FORMAT_INSTRUCTIONS = \"\"\"Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do (only one at a time)\n",
    "Action: the action to take, should be one of [Search, Calculator] \n",
    "Action Input: the input to the action\n",
    "\n",
    "then you will get an observation\n",
    "if your Thought is:  I now know the final answer\n",
    "you should reply\n",
    "Final Answer: the final answer to the original input question\"\"\"\n",
    "\n",
    "SUFFIX = \"\"\"Stop after each Action call and wait to get the intermediate results, do not make up multiple steps by yourself\n",
    "\n",
    "example:\n",
    "Question:  How much money does the richest person in the world have?\n",
    "Though: I'll need to find the current richest person in the world, then i need to find the net worth of this person\n",
    "Action: Search\n",
    "Action input: richest person in the world\n",
    "\n",
    "Begin! <</SYS>>\n",
    "\n",
    "Question: {input} \n",
    "Thought:{agent_scratchpad}[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "<s>[INST] Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\n",
    "Calculator: Useful for when you need to answer questions about math.\n",
    "\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of Search, Calculator\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action (wait for an observation, you must not provide an observation)\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times Only if an observation was provided)\n",
    "Thought: I now know the final answer \n",
    "Final Answer: the final answer to the original input question Only if there is enough information to give a response\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: who is older? the president of America or PM of India. give me their difference in age \n",
    "Thought: [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"<s>[INST] You are a helpfull assistant who can use tools one at a time to find an answer. You have access to the following tools:\n",
    "\n",
    "Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\n",
    "Calculator: Useful for when you need to answer questions about math.\n",
    "\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of Search, Calculator\n",
    "Action Input: the input to the action and then </s>\n",
    "Observation: the result of the action (wait for an observation, you must not provide an observation)\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times Only if an observation was provided)\n",
    "Final Answer: the final answer to the original input question, Only if there is enough information to give a response\n",
    "\n",
    "Don't try to make up observations  stop if an observation is needed\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: who is older? the president of America or PM of India. give me their difference in age \n",
    "Thought: [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"<s>[INST] You are a helpful assistant who can use tools one at a time to  get closer to the answer,. You have access to the following tools:\n",
    "\n",
    "Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\n",
    "Calculator: Useful for when you need to answer questions about math.\n",
    "\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of Search, Calculator\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action (wait for an observation, you must not provide an observation)\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times Only if an observation was provided)\n",
    "\n",
    "if there is not an answer based in observations yet reply (waiting for an observation) and finish your response\n",
    "if there is enough information to give a response reply\n",
    "Final Answer: the final answer to the original input question \n",
    "if not write (...)\n",
    "\n",
    "Don't try to make up observations  stop if an observation is needed\n",
    "Think step by step and be patient when answering.\n",
    "Only when you are done with all steps, provide the answer based on the intermediate steps.  \n",
    "Before presenting your final answer, make sure that you correctly waited for the observation steps. \n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: who is older? the president of America or PM of India. give me their difference in age \n",
    "Thought: [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"\"\"<s>[INST] You are a helpful assistant who can use tools one at a time to  get closer to the answer,. You have access to the following tools:\"\"\"\n",
    "\n",
    "FORMAT_INSTRUCTIONS = \"\"\"Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of {tool_names}\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action (wait for an observation, you must not provide an observation)\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times Only if an observation was provided)\n",
    "\n",
    "if there is not an answer based in observations yet reply (waiting for an observation) and finish your response\n",
    "if there is enough information to give a response reply\n",
    "Final Answer: the final answer to the original input question \n",
    "if not write (...)\n",
    "\"\"\"\n",
    "\n",
    "SUFFIX = \"\"\"Don't try to make up observations  stop if an observation is needed\n",
    "Think step by step and be patient when answering.\n",
    "Only when you are done with all steps, provide the answer based on the intermediate steps.  \n",
    "Before presenting your final answer, make sure that you correctly waited for the observation steps. \n",
    "\n",
    "Begin!\n",
    "\n",
    "Begin! <</SYS>>\n",
    "\n",
    "Question: {input} \n",
    "Thought:{agent_scratchpad}[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    #verbose=True,\n",
    "    max_iterations = 4,\n",
    "    agent_kwargs={\n",
    "        #'prefix': PREFIX, \n",
    "        #'format_instructions': FORMAT_INSTRUCTIONS,\n",
    "        #'suffix': SUFFIX\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\"hi how are you\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\"who is older? the president of America or PM of India. give me their difference in age\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\"who is the president of Colombia\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sambanova mistral response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\"who is older? the president of America or PM of India. give me their difference in age\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
